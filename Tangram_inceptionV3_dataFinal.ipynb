{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Tangram_inceptionV3_dataFinal.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sboomi/exploradome_tangram/blob/tf2---team-3/Tangram_inceptionV3_dataFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TBFXQGKYUc4X"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "1z4xy2gTUc4a",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FE7KNzPPVrVV"
      },
      "source": [
        "# Image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gN7G9GFmVrVY"
      },
      "source": [
        "Our objective is to classify tangram from images. We are going to build a deep learning with library TensorFlow2.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhVWMYTz2ckb",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zF9uvbXNVrVY"
      },
      "source": [
        "## Step 1 : Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VddxeYBEVrVZ"
      },
      "source": [
        "Let's start by importing the required packages. The `os` package is used to read files and directory structure, NumPy is used to convert python list to numpy array and to perform required matrix operations and `matplotlib.pyplot` to plot the graph and display images in the training and validation data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jlchl4x2VrVg"
      },
      "source": [
        "Import Tensorflow and the Keras classes needed to construct our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nmMfiSBcXZST",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.image as img\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import collections\n",
        "from collections import defaultdict\n",
        "from shutil import copy, copytree, rmtree\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D,Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow import keras\n",
        "\n",
        "import cv2\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "import itertools\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm6g8C8lbOVk",
        "colab_type": "text"
      },
      "source": [
        "# Nouvelle section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UZZI6lNkVrVm"
      },
      "source": [
        "## Step 2 :  Load data\n",
        "We assign variables with the proper file path for the training, validation set and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C1nqr-CYY6uw",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGxPhgDrVwAF",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sRucI3QqVrVy",
        "colab": {}
      },
      "source": [
        "PATH = '/content/drive/My Drive/data/final_dataset_balanced'\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "test_dir = os.path.join(PATH, 'test')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpqdGq9Euhm1",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Step3 : Data pre-processing and data augmentation\n",
        "we will \"augment\" our training via a number of random transformations, so that our model would never see twice the exact same picture. This helps prevent overfitting and helps the model generalize better.\n",
        "1. We will use the keras.preprocessing.image.ImageDataGenerator class. This class allows to configure random transformations and normalization operations to be done on your image data during training \n",
        "2. We will use .flow_from_directory() to generate batches of image data (and their labels) directly from our jpgs in their respective folders, applies rescaling, and resizes the images into the required dimensions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyP_Wk3x4dMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_classes = 12\n",
        "IMAGE_SIZE = (299, 299)\n",
        "batch_size = 31\n",
        "\n",
        "# Our original images consist in RGB coefficients in the 0-255, but such values would be too high for our models to process, so we target values between 0 and 1 instead by scaling with a 1/255.\n",
        "# width_shift_range, height_shift_range: (fraction of total height||width). Range for random horizontal shifts.\n",
        "# shear_range is for randomly applying shearing transformations (deformation from an angle)\n",
        "# fill_mode: points outside the boundaries of the input are filled according to the given mode\n",
        "# rotation_range: degree range for random rotations.\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rotation_range=50,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2, \n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest', validation_split=0.3)\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical', subset='training')\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical', subset='validation')\n",
        "\n",
        "# class_mode: set “binary” if you have only two classes to predict, if not set to“categorical” (her we have 12 classes to predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b5Ej-HLGVrWZ"
      },
      "source": [
        "## Step 4 : Create the model\n",
        "There are different ways to modulate entropic capacity. The main one is the choice of the number of parameters in your model, i.e. the number of layers and the size of each layer. To reduce  overfitting we wille use dropout. \n",
        "we would use an optimizer with a very slow learning rate. In general, SGD is good choice for this as opposed to adaptive methods like Adam etc.\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Qa3eneiEcXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the base pre-trained model\n",
        "inception = InceptionV3(weights='imagenet', include_top=False)\n",
        "# add a global spatial average pooling layer\n",
        "x = inception.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "#  let's add a fully-connected layer\n",
        "x = Dense(128,activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "# we have 12 classes\n",
        "predictions = Dense(12,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRCCkDI8Rk5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath='best_model_inceptionV3.h5', verbose=1, save_best_only=True)\n",
        "# model weights are saved at the end of every epoch, if it's the best seen\n",
        "\n",
        "csv_logger = CSVLogger('history_accuracy.log')\n",
        "# CSVLogger:  streams epoch results to a CSV file.\n",
        "\n",
        "callbacks=[csv_logger, checkpointer]\n",
        "# list of CSVFile and weights to stock the value to stop the training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB6NSTzcSTeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is the model we will train\n",
        "model = Model(inputs=inception.input, outputs=predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYctDUCLuyGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNssDBEilGjS",
        "colab_type": "text"
      },
      "source": [
        "### Model summary\n",
        "\n",
        "View all the layers of the network using the model's `summary` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuxIlYyN1-Eu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGGEcHTMk5T0",
        "colab_type": "text"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrDp9uJWB8kM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=30,\n",
        "    validation_data=validation_generator,\n",
        "    verbose=1,\n",
        "    callbacks = [callbacks],\n",
        "    workers=10)\n",
        "\n",
        "\n",
        "# callbacks: list of CSVFile and weights to stock the value to stop the training (to avoid overfitting)\n",
        "# workers: Sequence input only. Maximum number of processes to spin up when using process-based threading."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBk-Lq6xGRo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot to show the evolution of accuracy and loss during epochs\n",
        "\n",
        "def plot_accuracy_loss(history):\n",
        "    \"\"\"\n",
        "        Plot the accuracy and the loss during the training of the nn.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(10,5))\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(221)\n",
        "    plt.plot(history.history['accuracy'],'bo--', label = \"accuracy\")\n",
        "    plt.plot(history.history['val_accuracy'], 'ro--', label = \"val_accuracy\")\n",
        "    plt.title(\"train_accuracy vs val_accuracy\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot loss function\n",
        "    plt.subplot(222)\n",
        "    plt.plot(history.history['loss'],'bo--', label = \"loss\")\n",
        "    plt.plot(history.history['val_loss'], 'ro--', label = \"val_loss\")\n",
        "    plt.title(\"train_loss vs val_loss\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7HIXX6sGtQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_accuracy_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEhba0POwvZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255 ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDFgu5LPAGHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # all the test images should be placed inside a separate folder inside the test folder. \n",
        "\n",
        " # shuffle: False because we have to keep the order of the images\n",
        " # target_size: the size of our input images, every image will be resized to this size \n",
        " \n",
        "test_data_gen = test_datagen.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=test_dir,\n",
        "                                                              shuffle=False,\n",
        "                                                              target_size=IMAGE_SIZE,\n",
        "                                                              class_mode='categorical')\n",
        "                                                    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0MDOj9OqpRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred=model.predict_generator(test_data_gen,verbose=1,steps=960/batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKWL79P3qt1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# argmax to chose the greatest accuracy for the predicted class\n",
        "predicted_class_indices=np.argmax(pred,axis=1)\n",
        "\n",
        "# Keras gives indices to each classe\n",
        "# to have good indice with the good name class\n",
        "labels = (train_generator.class_indices)\n",
        "print(labels)\n",
        "labels = dict((v,k) for k,v in labels.items())\n",
        "predictions = [labels[k] for k in predicted_class_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXAk_V-q5pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to stock the informations about the prediction of each image into a dataframe\n",
        "\n",
        "filenames=test_data_gen.filenames\n",
        "results=pd.DataFrame({\"Filename\":filenames,\n",
        "                      \"Predictions\":predictions})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzofFVKN6BC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_pred = []\n",
        "\n",
        "for filename in filenames:\n",
        "  real_pred.append(filename.split('/')[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bGol-gA6E3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm = confusion_matrix(real_pred, predictions, labels=[\"bateau\", \"bol\", \"chat\", \"coeur\", \"cygne\", \"lapin\", \"maison\", \"marteau\", \"montagne\", \"pont\", \"renard\", \"tortue\"])\n",
        "\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCs2LUHp6KYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy_score(real_pred, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtbUyADd6XDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(9,9))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9ngO6HY6cyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=train_generator.class_indices, title='Confusion Matrix')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}